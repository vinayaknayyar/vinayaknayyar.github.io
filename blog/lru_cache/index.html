<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Demystifying the LRU Cache | Vinayak's Blog</title><link rel=icon href=#ZgotmplZ></link>
<link rel=stylesheet href=/css/styles.css><style>:root{--background-color:#feefc8ff;--links-color:#d03a58ff;--text-color:#4c4c4cff;--table-header-color:#cdf789ff;--table-content-color:#dcf6b3ff;--table-row-hover-color:#f6f2b3ff;--writing-card-background:#fdf1cd;--font-family:Lato;--header-color:#0a1f5dff;--paragraph-color:#333333}</style><link rel=preload href=https://rsms.me/><link rel=stylesheet href=https://rsms.me/inter/inter.css><link rel=stylesheet href=/css/custom.css></head><body><nav class=top-nav><a href=https://vinayaknayyar.github.io/ class=site-title>Vinayak's Blog</a><div class=nav-links><a href=/blog/>Blog</a></div></nav><main><div class=container><article class=single-post><header class=post-header><h1>Demystifying the LRU Cache</h1><div class=post-meta><time>December 31, 2025</time>
<span class=tags>| Tags: Operating System, Computer Architecture, DSA</span></div></header><div class=post-content><h2 id=problem-statement>Problem Statement</h2><p>In the world of high-performance computing, speed is everything. When applications scale, the bottleneck is often the time it takes to fetch data from slow storage layers. This is where caching comes into play.</p><h2 id=1-what-is-a-cache>1. What is a Cache?</h2><p>In computing, a cache is a small, fast storage layer that keeps copies of data so future requests for that data can be served faster. It sits between a slow data source (disk, network, database, main memory) and the consumer (CPU, application, browser), trading space for time.</p><p>Typical examples where caches are used:</p><ol><li>CPU caches (L1/L2/L3) storing recently used data to reduce main-memory(SRAM vs DRAM) latency.</li><li>Database caches holding hot query results or key–value objects in RAM.</li><li>Web and CDN caches storing HTTP responses close to users to reduce latency and save bandwidth.</li></ol><p>Caches matter because most workloads exhibit <strong><a href=https://en.wikipedia.org/wiki/Locality_of_reference>temporal locality</a></strong> and <strong><a href=https://en.wikipedia.org/wiki/Locality_of_reference>spatial locality</a></strong>; computer systems rely on the past as a predictor of what is likely to happen next, i.e. <em>&lsquo;recently accessed&rsquo;</em> or <em>&rsquo;nearby data&rsquo;</em> is likely to be accessed again soon, so keeping it close significantly improves throughput and tail latency[1].</p><h2 id=2-types-of-caching-strategies>2. Types of Caching Strategies</h2><p>A cache has finite capacity, so when it is full, it must decide <strong>which</strong> entry to evict to make room for new data, this decision policy is the cache eviction or cache replacement strategy.</p><p>Some widely used strategies include:</p><h4 id=fifo-first-in-first-out>FIFO (First In, First Out)</h4><ul><li>Evicts the oldest inserted item regardless of how often it was used.</li><li>Simple to implement but can perform poorly when old items are still hot.</li></ul><h4 id=random-replacement>Random replacement</h4><ul><li>Evicts a random entry.</li><li>Easy and cheap but may evict frequently used items by chance.</li></ul><h4 id=lru-least-recently-used>LRU (Least Recently Used)</h4><ul><li>Evicts the entry that has not been used for the longest time.</li><li>Exploits temporal locality and is often a good default in practice.</li></ul><h4 id=lfu-least-frequently-used>LFU (Least Frequently Used)</h4><ul><li>Evicts the item with the lowest access count.</li><li>Captures long‑term popularity but is more complex to maintain efficiently.</li></ul><p>Real systems (e.g., Redis, OS page replacement, web caches) often use LRU or approximations of it because it balances hit rate with implementation simplicity.</p><h2 id=3-what-is-an-lru-cache>3. What is an LRU Cache?</h2><p>An LRU (Least Recently Used) cache is a fixed‑capacity key–value store that always evicts the <strong>least recently accessed</strong> entry when it needs space. “Accessed” means either read (<code>get</code>) or write (<code>put/update</code>); every access marks that key as most recently used.</p><h3 id=designing-the-lru-cache>Designing the LRU cache</h3><p>As caching is all about speed, it&rsquo;s needless to say that we need a data structure (or a combination) that supports data storage, data retrieval and data eviction in the least possible time O(1).</p><p>Here&rsquo;s an analysis of different data structures and the Time Complexity(TC) for these operations</p><table><thead><tr><th>Data Structure</th><th>Store</th><th>Retrieve</th><th>Evict</th></tr></thead><tbody><tr><td>Array (Unsorted)</td><td>O(1) add, O(n) update(linear search)</td><td>O(n) linear search</td><td>O(1) if LRU is known index like front/end</td></tr><tr><td>Array (Sorted)</td><td>O(n) rearrangement reqd to keep array sorted</td><td>O(logn) binary search</td><td>O(1) if LRU is known index like front/end</td></tr><tr><td>Linked List (Singly/Doubly)</td><td>O(1) insert @ head/tail, O(n) update</td><td>O(n) linear search</td><td>O(1) if LRU is @ head/tail</td></tr><tr><td>Hash Map</td><td>O(1) insertion/update</td><td>O(1) lookup</td><td>O(n) to find LRU</td></tr><tr><td>Balanced BST</td><td>O(logn) insertion/update</td><td>O(logn) lookup</td><td>O(logn) to find LRU</td></tr></tbody></table><p>No single structure satisfies all the three requirements. Therefore we go with a combination of Linked list and Hashmap that together satisfy all the three operations in O(1) when properly/correctly used.</p><h3 id=how-it-works>How it Works:</h3><p>To achieve O(1) TC for both <code>put/update</code> and <code>get</code> operations, we utilise the combination as below:</p><ol><li>A Doubly Linked List: To maintain the order of usage[2]. The front is the Most Recently Used (MRU), and the back is the Least Recently Used (LRU).</li><li>A Hash Map (unordered_map): To provide fast access to the nodes in the linked list using a key.</li></ol><h4 id=highlevel-behavior>High‑level behavior:</h4><h5 id=on-getkey>On <code>get(key)</code></h5><ol><li>If the key exists, return its value and mark that entry as most recently used.</li><li>If not, report a miss (commonly via <code>std::optional</code> or sentinel value).</li></ol><h5 id=on-putkey-value>On <code>put(key, value)</code></h5><ol><li>If the key already exists, update its value and mark it as most recently used.</li><li>If it does not exist and the cache is not full, insert it as most recently used.</li><li>If the cache is full, evict the least recently used entry, then insert the new one as most recently used.</li></ol><p>To support both O(1) lookups and O(1) updates of the “recency” order, the classic implementation combines:</p><ol><li>A hash table (<code>std::unordered_map</code>) mapping keys to nodes.</li><li>A doubly linked list (<code>std::list</code>) ordered from most recently used (front) to least recently used (back).</li></ol><h2 id=4-implementation-of-lru-cache-in-c-11>4. Implementation of LRU Cache in C++ 11</h2><p>Below is a possible implementations using <code>std::list</code> and <code>std::unordered_map</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;iostream&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;unordered_map&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;list&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;utility&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LRUCache</span> {
</span></span><span style=display:flex><span><span style=color:#66d9ef>private</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> capacity;
</span></span><span style=display:flex><span>    <span style=color:#75715e>// List stores pairs of {key, value}
</span></span></span><span style=display:flex><span>    std<span style=color:#f92672>::</span>list<span style=color:#f92672>&lt;</span>std<span style=color:#f92672>::</span>pair<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span>, <span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;&gt;</span> cacheList;
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Map stores key -&gt; iterator to the node in cacheList
</span></span></span><span style=display:flex><span>    std<span style=color:#f92672>::</span>unordered_map<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span>, std<span style=color:#f92672>::</span>list<span style=color:#f92672>&lt;</span>std<span style=color:#f92672>::</span>pair<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>int</span>, <span style=color:#66d9ef>int</span><span style=color:#f92672>&gt;&gt;::</span>iterator<span style=color:#f92672>&gt;</span> cacheMap;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>public</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>    LRUCache(<span style=color:#66d9ef>int</span> cap) <span style=color:#f92672>:</span> capacity(cap) {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> <span style=color:#a6e22e>get</span>(<span style=color:#66d9ef>int</span> key) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (cacheMap.find(key) <span style=color:#f92672>==</span> cacheMap.end()) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>; <span style=color:#75715e>// Key not found
</span></span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Move the accessed item to the front (MRU)
</span></span></span><span style=display:flex><span>        cacheList.splice(cacheList.begin(), cacheList, cacheMap[key]);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> cacheMap[key]<span style=color:#f92672>-&gt;</span>second;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>put</span>(<span style=color:#66d9ef>int</span> key, <span style=color:#66d9ef>int</span> value) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (cacheMap.find(key) <span style=color:#f92672>!=</span> cacheMap.end()) {
</span></span><span style=display:flex><span>            <span style=color:#75715e>// Key exists: update value and move to front
</span></span></span><span style=display:flex><span>            cacheList.splice(cacheList.begin(), cacheList, cacheMap[key]);
</span></span><span style=display:flex><span>            cacheMap[key]<span style=color:#f92672>-&gt;</span>second <span style=color:#f92672>=</span> value;
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (cacheList.size() <span style=color:#f92672>==</span> capacity) {
</span></span><span style=display:flex><span>            <span style=color:#75715e>// Cache full: remove the LRU item (from the back)
</span></span></span><span style=display:flex><span>            <span style=color:#66d9ef>int</span> lastKey <span style=color:#f92672>=</span> cacheList.back().first;
</span></span><span style=display:flex><span>            cacheList.pop_back();
</span></span><span style=display:flex><span>            cacheMap.erase(lastKey);
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Add new item to the front
</span></span></span><span style=display:flex><span>        cacheList.push_front({key, value});
</span></span><span style=display:flex><span>        cacheMap[key] <span style=color:#f92672>=</span> cacheList.begin();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><h2 id=5-validating-the-functionality>5. Validating the functionality</h2><p>Using Google Test (GTest) to validate the cache behaviour under various scenarios.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;gtest/gtest.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Test case for basic Put and Get
</span></span></span><span style=display:flex><span>TEST(LRUCacheTest, BasicOperations) {
</span></span><span style=display:flex><span>    LRUCache <span style=color:#a6e22e>cache</span>(<span style=color:#ae81ff>2</span>);
</span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>10</span>);
</span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>20</span>);
</span></span><span style=display:flex><span>    EXPECT_EQ(cache.get(<span style=color:#ae81ff>1</span>), <span style=color:#ae81ff>10</span>);
</span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>30</span>);            <span style=color:#75715e>// Evicts key 2
</span></span></span><span style=display:flex><span>    EXPECT_EQ(cache.get(<span style=color:#ae81ff>2</span>), <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>); <span style=color:#75715e>// Should not be found
</span></span></span><span style=display:flex><span>    EXPECT_EQ(cache.get(<span style=color:#ae81ff>3</span>), <span style=color:#ae81ff>30</span>);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Test case for updating existing keys
</span></span></span><span style=display:flex><span>TEST(LRUCacheTest, UpdateKey) {
</span></span><span style=display:flex><span>    LRUCache <span style=color:#a6e22e>cache</span>(<span style=color:#ae81ff>2</span>);
</span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>10</span>);
</span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>15</span>);
</span></span><span style=display:flex><span>    EXPECT_EQ(cache.get(<span style=color:#ae81ff>1</span>), <span style=color:#ae81ff>15</span>);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// Test case for LRU Eviction Logic
</span></span></span><span style=display:flex><span>TEST(LRUCacheTest, EvictionLogic) {
</span></span><span style=display:flex><span>    LRUCache <span style=color:#a6e22e>cache</span>(<span style=color:#ae81ff>2</span>);
</span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>);
</span></span><span style=display:flex><span>    cache.get(<span style=color:#ae81ff>1</span>);      <span style=color:#75715e>// Key 1 becomes MRU, Key 2 is now LRU
</span></span></span><span style=display:flex><span>    cache.put(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>);   <span style=color:#75715e>// Should evict Key 2
</span></span></span><span style=display:flex><span>    EXPECT_EQ(cache.get(<span style=color:#ae81ff>2</span>), <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>    EXPECT_EQ(cache.get(<span style=color:#ae81ff>1</span>), <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>(<span style=color:#66d9ef>int</span> argc, <span style=color:#66d9ef>char</span> <span style=color:#f92672>**</span>argv) {
</span></span><span style=display:flex><span>    <span style=color:#f92672>::</span>testing<span style=color:#f92672>::</span>InitGoogleTest(<span style=color:#f92672>&amp;</span>argc, argv);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> RUN_ALL_TESTS();
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=final-wrap-up>Final Wrap Up</h2><p>A cache is a fast but limited storage layer used to hide slow operations; choosing a good eviction policy is crucial for its effectiveness. The LRU cache is a cornerstone of systems design, balancing simplicity with high performance. By utilizing a Doubly Linked List for ordering and a Hash Map for O(1) lookups, we create a system that intelligently keeps the most relevant data at our fingertips.</p><p>Whether you are optimizing a database or building a high-traffic web server, understanding these caching fundamentals is essential for any software engineer.</p><h2 id=further-reading>Further Reading</h2><ol><li><a href=https://en.wikipedia.org/wiki/Cache_replacement_policies#LRU>LRU Cache - Wikipedia</a></li><li><a href=https://www.geeksforgeeks.org/lru-cache-implementation/>GeeksforGeeks: LRU Cache Implementation</a></li><li><a href=https://leetcode.com/problems/lru-cache/>LeetCode Problem 146: LRU Cache</a></li></ol><h2 id=references>References</h2><p>[1] <a href=https://www.geeksforgeeks.org/system-design/long-tail-latency-problem-in-microservices/>Tail Latency - GeeksforGeeks</a> Long-tail latency refers to the disproportionate impact of a small percentage of requests that take significantly longer to process than the majority of requests.</p><p>[2] <a href=https://en.cppreference.com/w/cpp/container/list/splice.html><code>std::list</code> Splicing - cppreferences</a> C++&rsquo;s std::list provides a member function called splice() which can transfer elements from one list to another, or reposition elements within the same list, with constant time complexity (O(1)) by simply reassigning internal pointers.</p></div></article></div></main><footer><div class=site-footer><div class=visitor-stats><div class=visitor-counter><h3>Visitor Statistics</h3><div class=flag-counter><a href=https://info.flagcounter.com/qGVn><img src=https://s01.flagcounter.com/count2/qGVn/bg_FFFFFF/txt_000000/border_CCCCCC/columns_3/maxflags_12/viewers_0/labels_1/pageviews_1/flags_0/percent_0/ alt="Flag Counter" border=0 style=max-width:100%;height:auto></a></div><div class=simple-counter style=margin-top:1rem><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fvinayaknayyar.github.io&count_bg=%23D03A58&title_bg=%230A1F5D&icon=&icon_color=%23E7E7E7&title=Visits&edge_flat=false" alt="Visitor Count" style=max-width:100%;height:auto></div></div></div><style>.visitor-stats{margin:2rem 0;padding:1.5rem;background-color:var(--writing-card-background,#fdf1cd);border-radius:8px;text-align:center}.visitor-stats h3{margin-top:0;color:var(--header-color,#000000);font-size:1.2rem;margin-bottom:1rem}.flag-counter,.simple-counter{display:flex;justify-content:center;align-items:center}.flag-counter img,.simple-counter img{border-radius:4px}</style><div class=footer-links><a href=https://github.com/vinayaknayyar target=_blank><span class="icon github-icon"></span>
</a><a href=https://in.linkedin.com/in/vinayak-nayyar><span class="icon linkedin-icon"></span></a></div></div></footer></body></html>